{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Div element not found.\n",
      "Div element not found.\n",
      "Div element not found.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "url = 'https://talentedge.com/iit-delhi/certificate-program-industrial-design-innovation-entrepreneurship'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    data1 = soup.find('h1', class_ = 'pl-title')\n",
    "    title = data1.text\n",
    "\n",
    "    data2 = soup.find('div', class_='desc_less')\n",
    "    if data2:\n",
    "        paragraphs = data2.find_all('p') \n",
    "        if len(paragraphs) >= 1:\n",
    "            description = paragraphs[0].text\n",
    "        else:\n",
    "            description = print(\"Not enough paragraphs found.\")\n",
    "    else:\n",
    "        description = print(\"Div element not found.\")\n",
    "\n",
    "    data3 = soup.find('div', class_='duration-of-course')\n",
    "    if data3:\n",
    "        paragraphs = data3.find_all('p') \n",
    "        if len(paragraphs) >= 2:\n",
    "            duration = paragraphs[0].text[9::]\n",
    "        else:\n",
    "            duration = print(\"Not enough paragraphs found.\")\n",
    "    else:\n",
    "        duration = print(\"Div element not found.\")\n",
    "    \n",
    "    data4 = soup.find('div', class_='duration-of-course')\n",
    "    if data4:\n",
    "        paragraphs = data4.find_all('p') \n",
    "        if len(paragraphs) >= 3:\n",
    "            timing = paragraphs[1].text\n",
    "        else:\n",
    "            timing = print(\"Not enough paragraphs found.\")\n",
    "    else:\n",
    "        timing = print(\"Div element not found.\")\n",
    "\n",
    "    data5 = soup.find('div', class_='duration-of-course')\n",
    "    if data5:\n",
    "        paragraphs = data5.find_all('p') \n",
    "        if len(paragraphs) >= 4:\n",
    "            course_start_date = paragraphs[2].text\n",
    "        else:\n",
    "            course_start_date = print(\"Not enough paragraphs found.\")\n",
    "    else:\n",
    "        course_start_date = print(\"Div element not found.\")\n",
    "\n",
    "    data6 = soup.find('div', class_= 'pl-deeper-undstnd to_flex_ul')\n",
    "    if data6:\n",
    "        ul_tags = data6.find_all('ul')  \n",
    "\n",
    "    for ul in ul_tags:\n",
    "        li_items = ul.find_all('li')  \n",
    "\n",
    "        result = ' | '.join([li.get_text().strip() for li in li_items])\n",
    "        what_will_you_learn = result\n",
    "\n",
    "    data7 = soup.find('div', class_= 'key-skills-sec')\n",
    "    if data7:\n",
    "        ul_tags = data7.find_all('ul')  \n",
    "\n",
    "    for ul in ul_tags:\n",
    "        li_items = ul.find_all('li') \n",
    "\n",
    "        result = ' | '.join([li.get_text().strip() for li in li_items])\n",
    "        skills = result\n",
    "\n",
    "    data8 = soup.find('h4', class_='cs-titlec')\n",
    "    data8_text = data8.text\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', data8_text.strip())\n",
    "    target_students = cleaned_text\n",
    "\n",
    "# data9 = soup.find('div', class_='eligible-right-top-list')\n",
    "\n",
    "# if data9:\n",
    "#     p_tags = data9.find_all('p')\n",
    "\n",
    "#     if p_tags:\n",
    "#         extracted_data = '|'.join(p_tag.get_text().strip() for p_tag in p_tags)\n",
    "#         eligibility_criteria = extracted_data.replace('\\n', ' ')\n",
    "    \n",
    "    data10 = soup.find('div', class_='sylab-tab-ul')\n",
    "\n",
    "    if data10:\n",
    "        ul_tag = data10.find('ul')  \n",
    "\n",
    "    if ul_tag:\n",
    "        li_tags = ul_tag.find_all('li')  \n",
    "\n",
    "        content_list = [li.find('a').get_text().strip() for li in li_tags]\n",
    "\n",
    "        content = '|'.join(content_list)\n",
    "    else:\n",
    "        content = print(\"No 'ul' element found.\")\n",
    "\n",
    "    data11 = soup.find('h4', class_ = 'best-fname')\n",
    "    if data11:\n",
    "        cleaned_text = data11.get_text().strip()\n",
    "    faculty_1_name = cleaned_text\n",
    "\n",
    "    data12 = soup.find('div', class_ = 'best-fdetail')\n",
    "    if data12:\n",
    "        p_tag = data12.find('p') \n",
    "\n",
    "    if p_tag:\n",
    "        extracted_data = p_tag.get_text().strip()\n",
    "        faculty_1_designation = extracted_data\n",
    "    else:\n",
    "        faculty_1_designation = print(\"No p element found in the div.\")\n",
    "\n",
    "    data14 = soup.find('h4', class_ = 'about-ititle')\n",
    "    if data14:\n",
    "        cleaned_text = data14.get_text().strip()\n",
    "    institute_name = cleaned_text\n",
    "\n",
    "    data15 = soup.find('div', class_='program-details-total-pay-amt-right')\n",
    "    data15_text = data15.text\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', data15_text.strip())\n",
    "    fee_in_INR = cleaned_text\n",
    "\n",
    "    data16 = soup.find('div', class_=\"program-details-total-pay-amt d-flex align-items-center justify-content-between dolor\")\n",
    "    data16_text = data16.text\n",
    "    match = re.search(r'USD\\s+(\\d+)', data16_text)\n",
    "    if match:\n",
    "        fee_in_USD = f\"USD {match.group(1)}\"\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to retrieve the page with status code {response.status_code}\")\n",
    "\n",
    "new_data3 = {\n",
    "    'Title': [title],\n",
    "    'Description': [description],\n",
    "    'Duration': [duration],\n",
    "    'Timing': [timing],\n",
    "    'Course Start Date ': [course_start_date],\n",
    "    'What will you learn' : [what_will_you_learn],\n",
    "    'Skills' : [skills],\n",
    "    'Target Students' : [target_students],\n",
    "    # 'Prerequisites / Eligibility criteria' : [eligibility_criteria],\n",
    "    'Content' : [content],\n",
    "    'Faculty 1 Name' : [faculty_1_name],\n",
    "    'Faculty 1 Designation' : [faculty_1_designation],\n",
    "    'Institute Name' : [institute_name],\n",
    "    'Fee in INR' : [fee_in_INR],\n",
    "    'Fee in USD' : [fee_in_USD]\n",
    "}\n",
    "\n",
    "df_new = pd.DataFrame(new_data3)\n",
    "\n",
    "df_existing = pd.read_excel('aviral_gautam.xlsx')\n",
    "\n",
    "df_concatenated = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "\n",
    "df_concatenated.to_excel('aviral_gautam.xlsx', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
